{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macrodata Refinement (MDR) Quickstart\n",
    "\n",
    "This notebook provides a comprehensive introduction to the Macrodata Refinement (MDR) library. MDR is a powerful toolkit for refining, validating, and transforming macrodata through various statistical and analytical methods.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Installation](#1.-Setup-and-Installation)\n",
    "2. [Generating Sample Data](#2.-Generating-Sample-Data)\n",
    "3. [Data Validation](#3.-Data-Validation)\n",
    "4. [Data Refinement](#4.-Data-Refinement)\n",
    "5. [Data Transformation](#5.-Data-Transformation)\n",
    "6. [Visualization](#6.-Visualization)\n",
    "7. [File I/O](#7.-File-I-O)\n",
    "8. [End-to-End Workflow](#8.-End-to-End-Workflow)\n",
    "9. [Advanced Usage](#9.-Advanced-Usage)\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's import the necessary packages and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install MDR if not already installed\n",
    "# !pip install macrodata-refinement\n",
    "\n",
    "# Standard imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "\n",
    "# Set up plotting\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# MDR imports\n",
    "from mdr.core.refinement import RefinementConfig, refine_data, apply_refinement_pipeline\n",
    "from mdr.core.validation import validate_data\n",
    "from mdr.core.transformation import transform_data\n",
    "from mdr.io.readers import read_csv, read_json\n",
    "from mdr.io.writers import write_csv, write_json\n",
    "from mdr.visualization.plots import (\n",
    "    plot_time_series,\n",
    "    plot_histogram,\n",
    "    plot_boxplot,\n",
    "    plot_refinement_comparison,\n",
    "    plot_validation_results,\n",
    "    save_plot,\n",
    "    PlotConfig\n",
    ")\n",
    "from mdr.utils.logging import setup_logger, get_logger, LogLevel\n",
    "from mdr.utils.helpers import detect_seasonality, interpolate_missing\n",
    "\n",
    "# Set up logging\n",
    "setup_logger(level=LogLevel.INFO)\n",
    "logger = get_logger()\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"mdr_notebook_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generating Sample Data\n",
    "\n",
    "Let's generate some sample data to work with. We'll create three variables with different characteristics:\n",
    "\n",
    "- `temperature`: A sinusoidal pattern with some outliers and missing values\n",
    "- `pressure`: A relatively stable variable with some drift and a few outliers\n",
    "- `humidity`: A variable with strong seasonality and more noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_data(num_points: int = 100, seed: int = 42) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generate sample data for demonstration purposes.\n",
    "    \n",
    "    Args:\n",
    "        num_points: Number of data points to generate\n",
    "        seed: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of variable names to arrays\n",
    "    \"\"\"\n",
    "    assert isinstance(num_points, int), \"num_points must be an integer\"\n",
    "    assert num_points > 0, \"num_points must be positive\"\n",
    "    assert isinstance(seed, int), \"seed must be an integer\"\n",
    "    \n",
    "    # Set random seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate time points\n",
    "    time = np.linspace(0, 10, num_points)\n",
    "    \n",
    "    # Generate temperature data (sinusoidal with outliers and missing values)\n",
    "    temperature = 20.0 + 5.0 * np.sin(time) + 0.5 * np.random.randn(num_points)\n",
    "    \n",
    "    # Add outliers to temperature\n",
    "    temperature[10] = 45.0  # High outlier\n",
    "    temperature[30] = 5.0   # Low outlier\n",
    "    \n",
    "    # Add missing values to temperature\n",
    "    temperature[20] = np.nan\n",
    "    temperature[40:43] = np.nan\n",
    "    \n",
    "    # Generate pressure data (stable with drift)\n",
    "    pressure = 101.3 + 0.01 * time + 0.05 * np.random.randn(num_points)\n",
    "    \n",
    "    # Add outliers to pressure\n",
    "    pressure[25] = 102.5  # High outlier\n",
    "    pressure[60] = 100.0  # Low outlier\n",
    "    \n",
    "    # Add missing values to pressure\n",
    "    pressure[15] = np.nan\n",
    "    pressure[50:53] = np.nan\n",
    "    \n",
    "    # Generate humidity data (seasonal with more noise)\n",
    "    humidity = 50.0 + 15.0 * np.sin(time/2) + 3.0 * np.random.randn(num_points)\n",
    "    \n",
    "    # Add outliers to humidity\n",
    "    humidity[5] = 95.0   # High outlier\n",
    "    humidity[55] = 10.0  # Low outlier\n",
    "    \n",
    "    # Add missing values to humidity\n",
    "    humidity[35] = np.nan\n",
    "    humidity[70:73] = np.nan\n",
    "    \n",
    "    return {\n",
    "        \"time\": time,\n",
    "        \"temperature\": temperature,\n",
    "        \"pressure\": pressure,\n",
    "        \"humidity\": humidity\n",
    "    }\n",
    "\n",
    "# Generate the sample data\n",
    "data = generate_sample_data(num_points=100)\n",
    "\n",
    "# Extract time array and remove from data dictionary\n",
    "time = data.pop(\"time\")\n",
    "\n",
    "# Display the first few values of each variable\n",
    "pd.DataFrame(data).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the raw data to see what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot configuration\n",
    "config = PlotConfig(\n",
    "    title=\"Raw Data\",\n",
    "    xlabel=\"Time\",\n",
    "    ylabel=\"Value\",\n",
    "    figsize=(12.0, 6.0),\n",
    "    grid=True\n",
    ")\n",
    "\n",
    "# Plot the time series\n",
    "fig, ax = plot_time_series(data, time, config=config)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the distribution of each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot configuration\n",
    "config = PlotConfig(\n",
    "    title=\"Data Distributions\",\n",
    "    xlabel=\"Value\",\n",
    "    ylabel=\"Frequency\",\n",
    "    figsize=(12.0, 6.0),\n",
    "    grid=True\n",
    ")\n",
    "\n",
    "# Plot histograms\n",
    "fig, ax = plot_histogram(data, bins=20, config=config)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can examine the box plots to identify potential outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot configuration\n",
    "config = PlotConfig(\n",
    "    title=\"Box Plots\",\n",
    "    xlabel=\"Variable\",\n",
    "    ylabel=\"Value\",\n",
    "    figsize=(12.0, 6.0),\n",
    "    grid=True\n",
    ")\n",
    "\n",
    "# Plot box plots\n",
    "fig, ax = plot_boxplot(data, config=config)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Validation\n",
    "\n",
    "Now that we have our data, let's validate it to identify any issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the data\n",
    "validation_results = validate_data(\n",
    "    data,\n",
    "    checks=[\"range\", \"missing\", \"outliers\"],\n",
    "    params={\n",
    "        \"range\": {\n",
    "            \"min_value\": 0.0,\n",
    "            \"max_value\": 150.0\n",
    "        },\n",
    "        \"missing\": {\n",
    "            \"threshold\": 0.05  # Allow up to 5% missing values\n",
    "        },\n",
    "        \"outliers\": {\n",
    "            \"threshold\": 2.5,  # Z-score threshold for outliers\n",
    "            \"method\": \"zscore\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Print validation results\n",
    "for var_name, result in validation_results.items():\n",
    "    print(f\"Variable: {var_name}\")\n",
    "    print(f\"  Valid: {result.is_valid}\")\n",
    "    \n",
    "    if not result.is_valid:\n",
    "        print(\"  Error messages:\")\n",
    "        for msg in result.error_messages:\n",
    "            print(f\"    - {msg}\")\n",
    "    \n",
    "    if result.statistics:\n",
    "        print(\"  Statistics:\")\n",
    "        for key, value in result.statistics.items():\n",
    "            print(f\"    - {key}: {value}\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the validation results to get a better understanding of the issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert validation results to a format suitable for plotting\n",
    "plot_results = {}\n",
    "for var_name, result in validation_results.items():\n",
    "    plot_results[var_name] = {\n",
    "        \"is_valid\": result.is_valid,\n",
    "        \"error_messages\": result.error_messages,\n",
    "        \"statistics\": result.statistics or {}\n",
    "    }\n",
    "\n",
    "# Plot validation results\n",
    "fig, axes = plot_validation_results(plot_results)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Refinement\n",
    "\n",
    "Now that we have identified issues in our data, let's refine it to address these problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a refinement configuration\n",
    "config = RefinementConfig(\n",
    "    smoothing_factor=0.2,       # Smoothing factor (0-1, higher = less smoothing)\n",
    "    outlier_threshold=2.5,      # Z-score threshold for outlier detection\n",
    "    imputation_method=\"linear\", # Method for imputing missing values\n",
    "    normalization_type=\"minmax\" # Type of normalization to apply\n",
    ")\n",
    "\n",
    "# Refine the data\n",
    "refined_data = apply_refinement_pipeline(data, config)\n",
    "\n",
    "# Compare raw and refined data shape\n",
    "print(\"Raw data shape:\")\n",
    "for var_name, values in data.items():\n",
    "    print(f\"  {var_name}: {values.shape}\")\n",
    "\n",
    "print(\"\\nRefined data shape:\")\n",
    "for var_name, values in refined_data.items():\n",
    "    print(f\"  {var_name}: {values.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize the raw and refined data for each variable to see the effect of our refinement process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize refinement for each variable\n",
    "for var_name in data.keys():\n",
    "    # Plot refinement comparison\n",
    "    fig, axes = plot_refinement_comparison(\n",
    "        data[var_name],\n",
    "        refined_data[var_name],\n",
    "        time,\n",
    "        config=PlotConfig(title=f\"{var_name} Refinement Comparison\")\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Transformation\n",
    "\n",
    "Now that we have refined our data, let's apply some transformations to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transformations = [\n",
    "    # First normalize the data to [0, 1] range\n",
    "    {\"type\": \"normalize\", \"method\": \"minmax\"},\n",
    "    \n",
    "    # Then scale it to [1, 3] range (multiply by 2 and add 1)\n",
    "    {\"type\": \"scale\", \"factor\": 2.0, \"offset\": 1.0}\n",
    "]\n",
    "\n",
    "# Apply transformations to each variable\n",
    "transformed_data = {}\n",
    "for var_name, values in refined_data.items():\n",
    "    transformed_data[var_name] = transform_data(values, transformations)\n",
    "\n",
    "# Compare the data statistics before and after transformation\n",
    "for var_name in data.keys():\n",
    "    raw = data[var_name]\n",
    "    refined = refined_data[var_name]\n",
    "    transformed = transformed_data[var_name]\n",
    "    \n",
    "    print(f\"Variable: {var_name}\")\n",
    "    print(f\"  Raw:         min={np.nanmin(raw):.2f}, max={np.nanmax(raw):.2f}, mean={np.nanmean(raw):.2f}, std={np.nanstd(raw):.2f}\")\n",
    "    print(f\"  Refined:     min={np.min(refined):.2f}, max={np.max(refined):.2f}, mean={np.mean(refined):.2f}, std={np.std(refined):.2f}\")\n",
    "    print(f\"  Transformed: min={np.min(transformed):.2f}, max={np.max(transformed):.2f}, mean={np.mean(transformed):.2f}, std={np.std(transformed):.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot configuration\n",
    "config = PlotConfig(\n",
    "    title=\"Transformed Data\",\n",
    "    xlabel=\"Time\",\n",
    "    ylabel=\"Value\",\n",
    "    figsize=(12.0, 6.0),\n",
    "    grid=True\n",
    ")\n",
    "\n",
    "# Plot the transformed time series\n",
    "fig, ax = plot_time_series(transformed_data, time, config=config)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization\n",
    "\n",
    "MDR provides various visualization functions. Let's explore some of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a correlation matrix of the refined data\n",
    "from mdr.visualization.plots import plot_correlation_matrix\n",
    "\n",
    "# Create a DataFrame from the refined data\n",
    "refined_df = pd.DataFrame(refined_data)\n",
    "\n",
    "# Plot correlation matrix\n",
    "fig, ax = plot_correlation_matrix(\n",
    "    refined_df,\n",
    "    method=\"pearson\",\n",
    "    cmap=\"coolwarm\",\n",
    "    config=PlotConfig(title=\"Correlation Matrix\")\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot of temperature vs. humidity\n",
    "from mdr.visualization.plots import plot_scatter\n",
    "\n",
    "# Plot scatter\n",
    "fig, ax = plot_scatter(\n",
    "    refined_data[\"temperature\"],\n",
    "    refined_data[\"humidity\"],\n",
    "    config=PlotConfig(\n",
    "        title=\"Temperature vs. Humidity\",\n",
    "        xlabel=\"Temperature (Â°C)\",\n",
    "        ylabel=\"Humidity (%)\"\n",
    "    )\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap of the data over time (resampled)\n",
    "from mdr.visualization.plots import plot_heatmap\n",
    "\n",
    "# Create a resampled view of the data (10 time periods x 3 variables)\n",
    "num_periods = 10\n",
    "period_length = len(time) // num_periods\n",
    "heatmap_data = np.zeros((num_periods, 3))\n",
    "\n",
    "for i in range(num_periods):\n",
    "    start_idx = i * period_length\n",
    "    end_idx = (i + 1) * period_length\n",
    "    \n",
    "    heatmap_data[i, 0] = np.mean(refined_data[\"temperature\"][start_idx:end_idx])\n",
    "    heatmap_data[i, 1] = np.mean(refined_data[\"pressure\"][start_idx:end_idx])\n",
    "    heatmap_data[i, 2] = np.mean(refined_data[\"humidity\"][start_idx:end_idx])\n",
    "\n",
    "# Create row and column labels\n",
    "row_labels = [f\"Period {i+1}\" for i in range(num_periods)]\n",
    "col_labels = [\"Temperature\", \"Pressure\", \"Humidity\"]\n",
    "\n",
    "# Plot heatmap\n",
    "fig, ax = plot_heatmap(\n",
    "    heatmap_data,\n",
    "    row_labels=row_labels,\n",
    "    col_labels=col_labels,\n",
    "    cmap=\"viridis\",\n",
    "    config=PlotConfig(title=\"Data Heatmap by Time Period\")\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. File I/O\n",
    "\n",
    "MDR provides functions for reading and writing data in various formats. Let's demonstrate some of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the refined data to CSV\n",
    "csv_path = os.path.join(output_dir, \"refined_data.csv\")\n",
    "\n",
    "# Add time to the refined data for CSV export\n",
    "refined_with_time = refined_data.copy()\n",
    "refined_with_time[\"time\"] = time\n",
    "\n",
    "# Write to CSV\n",
    "write_csv(refined_with_time, csv_path)\n",
    "print(f\"Refined data saved to: {csv_path}\")\n",
    "\n",
    "# Read the CSV file back\n",
    "read_data = read_csv(csv_path)\n",
    "print(f\"Read data shape: {len(read_data)} variables\")\n",
    "print(f\"Variables: {list(read_data.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the transformed data to JSON\n",
    "json_path = os.path.join(output_dir, \"transformed_data.json\")\n",
    "\n",
    "# Add time to the transformed data for JSON export\n",
    "transformed_with_time = transformed_data.copy()\n",
    "transformed_with_time[\"time\"] = time\n",
    "\n",
    "# Write to JSON\n",
    "write_json(transformed_with_time, json_path)\n",
    "print(f\"Transformed data saved to: {json_path}\")\n",
    "\n",
    "# Read the JSON file back\n",
    "read_json_data = read_json(json_path)\n",
    "print(f\"Read data shape: {len(read_json_data)} variables\")\n",
    "print(f\"Variables: {list(read_json_data.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. End-to-End Workflow\n",
    "\n",
    "Now let's put everything together in an end-to-end workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data: Dict[str, np.ndarray], time: np.ndarray) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Process data through a complete MDR workflow.\n",
    "    \n",
    "    Args:\n",
    "        data: Dictionary of variable names to data arrays\n",
    "        time: Array of time points\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of processed data\n",
    "    \"\"\"\n",
    "    assert isinstance(data, dict), \"data must be a dictionary\"\n",
    "    assert all(isinstance(k, str) for k in data.keys()), \"data keys must be strings\"\n",
    "    assert all(isinstance(v, np.ndarray) for v in data.values()), \"data values must be numpy arrays\"\n",
    "    assert isinstance(time, np.ndarray), \"time must be a numpy array\"\n",
    "    \n",
    "    # Step 1: Validate the data\n",
    "    print(\"Step 1: Validating data...\")\n",
    "    validation_results = validate_data(\n",
    "        data,\n",
    "        checks=[\"range\", \"missing\", \"outliers\"]\n",
    "    )\n",
    "    \n",
    "    # Print validation summary\n",
    "    valid_count = sum(1 for result in validation_results.values() if result.is_valid)\n",
    "    print(f\"Validation summary: {valid_count}/{len(validation_results)} variables passed validation\")\n",
    "    \n",
    "    # Step 2: Refine the data\n",
    "    print(\"\\nStep 2: Refining data...\")\n",
    "    config = RefinementConfig(\n",
    "        smoothing_factor=0.2,\n",
    "        outlier_threshold=2.5,\n",
    "        imputation_method=\"linear\",\n",
    "        normalization_type=\"minmax\"\n",
    "    )\n",
    "    \n",
    "    refined_data = apply_refinement_pipeline(data, config)\n",
    "    print(f\"Refinement complete. Processed {len(refined_data)} variables.\")\n",
    "    \n",
    "    # Step 3: Verify refinement results\n",
    "    print(\"\\nStep 3: Verifying refinement results...\")\n",
    "    for var_name, values in refined_data.items():\n",
    "        # Check for missing values\n",
    "        missing_count = np.isnan(values).sum()\n",
    "        print(f\"  {var_name}: {missing_count} missing values\")\n",
    "        \n",
    "        # Check for extreme outliers\n",
    "        z_scores = np.abs((values - np.mean(values)) / np.std(values))\n",
    "        extreme_outliers = (z_scores > 3.0).sum()\n",
    "        print(f\"  {var_name}: {extreme_outliers} extreme outliers\")\n",
    "    \n",
    "    # Step 4: Apply transformations\n",
    "    print(\"\\nStep 4: Applying transformations...\")\n",
    "    transformations = [\n",
    "        {\"type\": \"normalize\", \"method\": \"minmax\"},\n",
    "        {\"type\": \"scale\", \"factor\": 2.0, \"offset\": 1.0}\n",
    "    ]\n",
    "    \n",
    "    transformed_data = {}\n",
    "    for var_name, values in refined_data.items():\n",
    "        transformed_data[var_name] = transform_data(values, transformations)\n",
    "    \n",
    "    print(f\"Transformation complete. Processed {len(transformed_data)} variables.\")\n",
    "    \n",
    "    # Return the processed data\n",
    "    return transformed_data\n",
    "\n",
    "# Process our sample data\n",
    "processed_data = process_data(data, time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the raw, refined, and processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with 3 subplots (one for each variable)\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 12), sharex=True)\n",
    "\n",
    "variables = list(data.keys())\n",
    "titles = [\"Temperature\", \"Pressure\", \"Humidity\"]\n",
    "\n",
    "for i, (var_name, title) in enumerate(zip(variables, titles)):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Plot raw data\n",
    "    ax.plot(time, data[var_name], 'o-', alpha=0.5, label=\"Raw Data\")\n",
    "    \n",
    "    # Plot refined data\n",
    "    ax.plot(time, refined_data[var_name], 's-', alpha=0.7, label=\"Refined Data\")\n",
    "    \n",
    "    # Plot processed data\n",
    "    ax.plot(time, processed_data[var_name], '^-', alpha=0.7, label=\"Processed Data\")\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(\"Value\")\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "\n",
    "# Set common x-axis label\n",
    "axes[-1].set_xlabel(\"Time\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced Usage\n",
    "\n",
    "Now let's explore some more advanced features of MDR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1. Seasonality Detection\n",
    "\n",
    "MDR provides utilities for detecting seasonality in time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with clear seasonality\n",
    "num_points = 200\n",
    "t = np.linspace(0, 20, num_points)\n",
    "seasonal_period = 20  # Period of 20 points\n",
    "\n",
    "# Create a seasonal pattern with noise\n",
    "seasonal_data = np.sin(2 * np.pi * t / seasonal_period) + 0.2 * np.random.randn(num_points)\n",
    "\n",
    "# Plot the seasonal data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(t, seasonal_data)\n",
    "plt.title(\"Seasonal Data\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Detect seasonality\n",
    "is_seasonal, detected_period = detect_seasonality(\n",
    "    seasonal_data,\n",
    "    max_lag=50,     # Maximum lag to consider\n",
    "    threshold=0.3   # Correlation threshold\n",
    ")\n",
    "\n",
    "print(f\"Is seasonal: {is_seasonal}\")\n",
    "if is_seasonal:\n",
    "    print(f\"Detected period: {detected_period}\")\n",
    "    print(f\"Actual period: {seasonal_period}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2. Custom Transformations\n",
    "\n",
    "MDR allows you to apply custom transformations to your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required functions\n",
    "from mdr.core.transformation import (\n",
    "    apply_logarithmic_transform,\n",
    "    apply_power_transform,\n",
    "    transform_data\n",
    ")\n",
    "\n",
    "# Create sample data with positively skewed distribution\n",
    "skewed_data = np.random.lognormal(mean=0, sigma=1, size=1000)\n",
    "\n",
    "# Apply logarithmic transformation\n",
    "log_data = apply_logarithmic_transform(skewed_data, base=10.0, epsilon=1e-10)\n",
    "\n",
    "# Apply power transformation (square root, which is power=0.5)\n",
    "sqrt_data = apply_power_transform(skewed_data, power=0.5, preserve_sign=True)\n",
    "\n",
    "# Plot the original and transformed distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Original data\n",
    "axes[0].hist(skewed_data, bins=30)\n",
    "axes[0].set_title(\"Original Data\")\n",
    "axes[0].set_xlabel(\"Value\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Log-transformed data\n",
    "axes[1].hist(log_data, bins=30)\n",
    "axes[1].set_title(\"Log-Transformed Data\")\n",
    "axes[1].set_xlabel(\"Value\")\n",
    "\n",
    "# Square root transformed data\n",
    "axes[2].hist(sqrt_data, bins=30)\n",
    "axes[2].set_title(\"Square Root Transformed Data\")\n",
    "axes[2].set_xlabel(\"Value\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3. Custom Refinement Pipeline\n",
    "\n",
    "Let's create a custom refinement pipeline for a specific dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_refinement_pipeline(data: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Custom refinement pipeline for specific data needs.\n",
    "    \n",
    "    Args:\n",
    "        data: Dictionary of variable names to data arrays\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of refined data\n",
    "    \"\"\"\n",
    "    assert isinstance(data, dict), \"data must be a dictionary\"\n",
    "    assert all(isinstance(k, str) for k in data.keys()), \"data keys must be strings\"\n",
    "    assert all(isinstance(v, np.ndarray) for v in data.values()), \"data values must be numpy arrays\"\n",
    "    \n",
    "    # Import necessary functions\n",
    "    from mdr.core.refinement import remove_outliers, impute_missing_values, smooth_data\n",
    "    \n",
    "    # Create a copy of the input data\n",
    "    refined = {}\n",
    "    \n",
    "    for var_name, values in data.items():\n",
    "        # Step 1: Impute missing values (method depends on the variable)\n",
    "        if var_name == \"temperature\":\n",
    "            # For temperature, use linear interpolation\n",
    "            imputed = impute_missing_values(values, method=\"linear\")\n",
    "        elif var_name == \"pressure\":\n",
    "            # For pressure, use forward fill (last observation carried forward)\n",
    "            imputed = impute_missing_values(values, method=\"forward\")\n",
    "        else:\n",
    "            # For other variables, use mean imputation\n",
    "            imputed = impute_missing_values(values, method=\"mean\")\n",
    "        \n",
    "        # Step 2: Remove outliers (threshold depends on the variable)\n",
    "        if var_name == \"temperature\":\n",
    "            # Temperature can have more natural variation, so use a higher threshold\n",
    "            no_outliers = remove_outliers(imputed, threshold=3.0)\n",
    "        elif var_name == \"pressure\":\n",
    "            # Pressure should be more stable, so use a lower threshold\n",
    "            no_outliers = remove_outliers(imputed, threshold=2.0)\n",
    "        else:\n",
    "            # For other variables, use a medium threshold\n",
    "            no_outliers = remove_outliers(imputed, threshold=2.5)\n",
    "        \n",
    "        # Step 3: Smooth the data (factor depends on the variable)\n",
    "        if var_name == \"temperature\":\n",
    "            # Less smoothing for temperature\n",
    "            smoothed = smooth_data(no_outliers, factor=0.3)\n",
    "        elif var_name == \"pressure\":\n",
    "            # More smoothing for pressure\n",
    "            smoothed = smooth_data(no_outliers, factor=0.1)\n",
    "        else:\n",
    "            # Medium smoothing for other variables\n",
    "            smoothed = smooth_data(no_outliers, factor=0.2)\n",
    "        \n",
    "        # Store the refined variable\n",
    "        refined[var_name] = smoothed\n",
    "    \n",
    "    return refined\n",
    "\n",
    "# Apply the custom refinement pipeline to our data\n",
    "custom_refined = custom_refinement_pipeline(data)\n",
    "\n",
    "# Compare with the standard refinement pipeline\n",
    "standard_config = RefinementConfig(\n",
    "    smoothing_factor=0.2,\n",
    "    outlier_threshold=2.5,\n",
    "    imputation_method=\"linear\",\n",
    "    normalization_type=\"minmax\"\n",
    ")\n",
    "standard_refined = apply_refinement_pipeline(data, standard_config)\n",
    "\n",
    "# Plot the comparison\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 12), sharex=True)\n",
    "\n",
    "variables = list(data.keys())\n",
    "titles = [\"Temperature\", \"Pressure\", \"Humidity\"]\n",
    "\n",
    "for i, (var_name, title) in enumerate(zip(variables, titles)):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Plot raw data\n",
    "    ax.plot(time, data[var_name], 'o-', alpha=0.5, label=\"Raw Data\")\n",
    "    \n",
    "    # Plot standard refinement\n",
    "    ax.plot(time, standard_refined[var_name], 's-', alpha=0.7, label=\"Standard Refinement\")\n",
    "    \n",
    "    # Plot custom refinement\n",
    "    ax.plot(time, custom_refined[var_name], '^-', alpha=0.7, label=\"Custom Refinement\")\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(\"Value\")\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "\n",
    "# Set common x-axis label\n",
    "axes[-1].set_xlabel(\"Time\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored the key features of the Macrodata Refinement (MDR) library:\n",
    "\n",
    "1. Data Validation - Identifying issues in the data\n",
    "2. Data Refinement - Fixing issues like outliers and missing values\n",
    "3. Data Transformation - Converting data to a desired format or scale\n",
    "4. Visualization - Creating various plots to understand the data\n",
    "5. File I/O - Reading and writing data in different formats\n",
    "6. Advanced Features - Seasonality detection, custom transformations, etc.\n",
    "\n",
    "MDR provides a comprehensive toolkit for working with macrodata, making it easy to clean, validate, transform, and visualize your data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
